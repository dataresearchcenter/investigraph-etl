# Overview

**investigraph** is a framework for building [datasets](../concepts/dataset.md) for [FollowTheMoney](https://followthemoney.tech) data.

As **investigraph** can be considered as an [ETL-process](https://en.wikipedia.org/wiki/Extract,_transform,_load) for [FollowTheMoney data](../stack/followthemoney.md), the structure of the dataset pipelines roughly follows the three steps of such a pipeline: *extract, transform, load*.

The documentation in this section assumes you already checked out [the tutorial](../tutorial.md).

Most of the running behaviour of a specific pipeline is configured on a per-[dataset](../concepts/dataset.md) basis and/or via arguments given to a specific run of the pipeline.


## Configuration

Pipelines for datasets are stored in a `YAML` file. [Read more about config.yml](config.md)

## Stages

### Seed

This is an optional stage before the _extract_ stage to programmatically configure [Sources](../reference/source.md) that get passed to the _extract_ stage. This can involve glob patterns from remote uris or a script.

This stage is configured via the _optional_ `seed` key within the `config.yml`

[Seed stage documentation](../stages/seed.md)

### Extract

In the first step of a pipeline, we focus on getting one or more data sources and extracting data records from them that will eventually be passed to the _transform stage_.

This stage is configured via the `extract` key within the `config.yml`

[Extract stage documentation](../stages/extract.md)

The _Records_ that this stage creates are passed to the next stage, _transform_.

### Transform

This stage transforms the records generated by the previous _extract_ stage into [FollowTheMoney entities](../concepts/entity.md). It can use a defined _mapping_ that doesn't require coding skills or execute a python script.

This stage is configured via the `transform` key within the `config.yml`

[Transform stage documentation](../stages/transform.md)

The _Entities_ that this stage creates are passed to the next stage, _load_.

### Load

This stage writes the _Entities_ created in the previous _transform_ stage to a store for aggregation. This can be a persistent backend like sql or kvrocks or an in-memory store, which is the default.

This stage is configured via the _optional_ `load` key within the `config.yml`

[Load stage documentation](../stages/load.md)

### Export

This optional stage at the end of a dataset pipeline exports the _Entities_ from the store to a json file. The default (if using the in-memory store in the previous _load_ stage) exports the data to a local output directory:

- Entities: `./data/<dataset>/entities.ftm.json`
- [Dataset index and stats](../concepts/dataset.md): `./data/<dataset>/index.json`

This stage is configured via the _optional_ `export` key within the `config.yml`

[Export stage documentation](../stages/export.md)
